{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33a9008e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>Cleaned_Title</th>\n",
       "      <th>Cleaned_Content</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theguardian</td>\n",
       "      <td>https://www.theguardian.com/commentisfree/2023...</td>\n",
       "      <td>As the SNP loses its iron grip on Scotland, La...</td>\n",
       "      <td>9/4/2023</td>\n",
       "      <td>What has been seen cannot be unseen. Some imag...</td>\n",
       "      <td>snp loses iron grip scotland labour seize gold...</td>\n",
       "      <td>unseen image potent indelibly etched nation re...</td>\n",
       "      <td>snp loses iron grip scotland labour seize gold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asahi</td>\n",
       "      <td>https://www.asahi.com/ajw/articles/14880931</td>\n",
       "      <td>‘Abenomask’ giveaway details finally disclosed...</td>\n",
       "      <td>8/4/2023</td>\n",
       "      <td>Forced into a corner by a court, the governmen...</td>\n",
       "      <td>abenomask giveaway detail finally disclosed co...</td>\n",
       "      <td>forced corner court government grudgingly rele...</td>\n",
       "      <td>abenomask giveaway detail finally disclosed co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rthk</td>\n",
       "      <td>https://news.rthk.hk/rthk/en/component/k2/1695...</td>\n",
       "      <td>Families flock to cemeteries to mark Ching Ming</td>\n",
       "      <td>5/4/2023</td>\n",
       "      <td>Large numbers of people flocked to Hong Kong's...</td>\n",
       "      <td>family flock cemetery mark ching ming</td>\n",
       "      <td>large number people flocked hong kong cemetery...</td>\n",
       "      <td>family flock cemetery mark ching ming large nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>japantimes</td>\n",
       "      <td>https://www.japantimes.co.jp/news/2023/04/03/b...</td>\n",
       "      <td>Japan firms roll back COVID measures for new r...</td>\n",
       "      <td>3/4/2023</td>\n",
       "      <td>A number of companies throughout Japan decided...</td>\n",
       "      <td>japan firm roll covid measure recruit event</td>\n",
       "      <td>number company japan decided dispense covid he...</td>\n",
       "      <td>japan firm roll covid measure recruit event nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>foxnews</td>\n",
       "      <td>https://www.foxnews.com/opinion/i-treated-2000...</td>\n",
       "      <td>I treated 20,000 COVID patients and 3 years af...</td>\n",
       "      <td>2/4/2023</td>\n",
       "      <td>Fox News medical contributor Dr. Janette Neshe...</td>\n",
       "      <td>treated covid patient year lockdown</td>\n",
       "      <td>fox news medical contributor janette nesheiwat...</td>\n",
       "      <td>treated covid patient year lockdown fox news m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source                                                URL  \\\n",
       "0  theguardian  https://www.theguardian.com/commentisfree/2023...   \n",
       "1        asahi        https://www.asahi.com/ajw/articles/14880931   \n",
       "2         rthk  https://news.rthk.hk/rthk/en/component/k2/1695...   \n",
       "3   japantimes  https://www.japantimes.co.jp/news/2023/04/03/b...   \n",
       "4      foxnews  https://www.foxnews.com/opinion/i-treated-2000...   \n",
       "\n",
       "                                               Title      Date  \\\n",
       "0  As the SNP loses its iron grip on Scotland, La...  9/4/2023   \n",
       "1  ‘Abenomask’ giveaway details finally disclosed...  8/4/2023   \n",
       "2    Families flock to cemeteries to mark Ching Ming  5/4/2023   \n",
       "3  Japan firms roll back COVID measures for new r...  3/4/2023   \n",
       "4  I treated 20,000 COVID patients and 3 years af...  2/4/2023   \n",
       "\n",
       "                                             Content  \\\n",
       "0  What has been seen cannot be unseen. Some imag...   \n",
       "1  Forced into a corner by a court, the governmen...   \n",
       "2  Large numbers of people flocked to Hong Kong's...   \n",
       "3  A number of companies throughout Japan decided...   \n",
       "4  Fox News medical contributor Dr. Janette Neshe...   \n",
       "\n",
       "                                       Cleaned_Title  \\\n",
       "0  snp loses iron grip scotland labour seize gold...   \n",
       "1  abenomask giveaway detail finally disclosed co...   \n",
       "2              family flock cemetery mark ching ming   \n",
       "3        japan firm roll covid measure recruit event   \n",
       "4                treated covid patient year lockdown   \n",
       "\n",
       "                                     Cleaned_Content  \\\n",
       "0  unseen image potent indelibly etched nation re...   \n",
       "1  forced corner court government grudgingly rele...   \n",
       "2  large number people flocked hong kong cemetery...   \n",
       "3  number company japan decided dispense covid he...   \n",
       "4  fox news medical contributor janette nesheiwat...   \n",
       "\n",
       "                                                Text  \n",
       "0  snp loses iron grip scotland labour seize gold...  \n",
       "1  abenomask giveaway detail finally disclosed co...  \n",
       "2  family flock cemetery mark ching ming large nu...  \n",
       "3  japan firm roll covid measure recruit event nu...  \n",
       "4  treated covid patient year lockdown fox news m...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, requests, urllib.parse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def cleaning(data):\n",
    "    # Remove punctuation\n",
    "    data = re.sub(\"[^a-zA-Z]\", \" \", data)\n",
    "\n",
    "    # Lowercasing\n",
    "    data = data.lower()\n",
    "\n",
    "    # Tokenizing\n",
    "    words = word_tokenize(data)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_list = requests.get(\n",
    "        \"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\"\n",
    "    ).content\n",
    "    stopwords = set(stopwords_list.decode().splitlines())\n",
    "    words = [word for word in words if not word in stopwords]\n",
    "\n",
    "    # Lemmatizing\n",
    "    lem = WordNetLemmatizer()\n",
    "    words = [lem.lemmatize(word) for word in words]\n",
    "\n",
    "    # Back to a String\n",
    "    cleaned_data = \" \".join(words)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"news.csv\")\n",
    "df = df.head(5)\n",
    "df[\"Cleaned_Title\"] = df[\"Title\"].apply(cleaning)\n",
    "df[\"Cleaned_Content\"] = df[\"Content\"].apply(cleaning)\n",
    "df[\"Text\"] = df[\"Cleaned_Title\"] + \" \" + df[\"Cleaned_Content\"]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e95cf5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "519a690e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\h1enr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\h1enr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\h1enr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\h1enr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glasgow', 'Scotland', 'Unforgotten', 'Edinburgh', 'Scotland', 'Scotland', 'Scotland', 'Scotland', 'Scotland', 'Labour', 'England', 'Scotland', 'Labour', 'Rutherglen', 'Scotland']\n",
      "['Japan', 'Abenomasks', 'Audit']\n",
      "['Large', 'Hong Kong', 'Diamond Hill', 'Covid', 'Hong Kong']\n",
      "['Japan', 'Japan']\n",
      "['News', 'Narcan', 'Spanish', 'U.S.', 'Washington', 'United States', 'Normal', 'Americans', 'America', 'Americans', 'New York', 'New York', 'New York']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Function to extract GPE entities using NLTK NER\n",
    "def ner(data):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(data)\n",
    "    \n",
    "    # Part-of-speech tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Perform named entity recognition\n",
    "    chunks = nltk.ne_chunk(pos_tags)\n",
    "    \n",
    "    # Extract GPE entities\n",
    "    gpe_entities = []\n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"GPE\":\n",
    "            gpe_entities.append(\" \".join(word for word, tag in chunk.leaves()))\n",
    "    \n",
    "    # Return a list of all GPE entities found in the text\n",
    "    return gpe_entities\n",
    "\n",
    "df[\"Regions\"] = df[\"Content\"].apply(ner)\n",
    "\n",
    "for i in range(5):\n",
    "    print(df.loc[i].Regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "207bf884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glasgow', 'Scotland', 'UK', 'Scotland', 'Westminster', 'Edinburgh', 'Scotland', 'Scotland', 'Yousaf', 'Scotland', 'Scotland', 'Yousaf', 'Scotland', 'Westminster', 'Scotland', 'Westminster', 'England', 'Scotland', 'Scotland']\n",
      "['Kamiwaki', 'Japan']\n",
      "[\"Hong Kong's\", 'Diamond Hill', 'Hong Kong']\n",
      "['Japan', 'Japan']\n",
      "['U.S.', 'Washington, DC', 'the United States', 'IV', 'America', 'Philadelphia', 'New York', 'New York', 'New York']\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "\n",
    "def ner(data):\n",
    "    # Load the model\n",
    "    nlp = en_core_web_sm.load()\n",
    "    # Process the text with the model\n",
    "    doc = nlp(data)\n",
    "\n",
    "    # Return a list of all GPE entities found in the text\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "\n",
    "df[\"Regions\"] = df[\"Content\"].apply(lambda x: ner(x))\n",
    "\n",
    "for i in range(5):\n",
    "    print(df.loc[i].Regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0caa0120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h1enr\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:594: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-11 00:34:08,787 loading file C:\\Users\\h1enr\\.flair\\models\\ner-english-fast\\4c58e7191ff952c030b82db25b3694b58800b0e722ff15427f527e1631ed6142.e13c7c4664ffe2bbfa8f1f5375bd0dced866b8c1dd7ff89a6d705518abf0a611\n",
      "2023-04-11 00:34:10,363 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "['Glasgow', 'Scotland', 'UK', 'Westminster', 'Edinburgh', 'Scotland', 'Holyrood', 'Scotland', 'Westminster', 'Scotland', 'Bute House', 'Scotland', 'Westminster', 'Blackpool South', 'Scotland', 'Westminster', 'England', 'Scotland', 'Rutherglen', 'Hamilton West', 'Scotland']\n",
      "['Japan']\n",
      "['Hong Kong', 'Junk Bay', 'Wo Hop Shek', 'Diamond Hill', 'Hong Kong']\n",
      "['Japan', 'Japan']\n",
      "['U.S.', 'White House', 'Washington', 'DC', 'United States', 'America', 'Philadelphia', 'New York', 'New York', 'New York']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "def ner(df):\n",
    "    # load tagger\n",
    "    tagger = SequenceTagger.load(\"flair/ner-english-fast\")\n",
    "\n",
    "    # create an empty list to store all the LOC entities\n",
    "    locs_all = []\n",
    "\n",
    "    # process each row of the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # get the text from the 'Content' column\n",
    "        text = row[\"Content\"]\n",
    "        \n",
    "        \n",
    "\n",
    "        # create a new sentence object\n",
    "        sentence = Sentence(text)\n",
    "        \n",
    "\n",
    "        # predict NER tags\n",
    "        tagger.predict(sentence)\n",
    "\n",
    "        # extract LOC entities\n",
    "        locs = [\n",
    "            entity.text for entity in sentence.get_spans(\"ner\") if entity.tag == \"LOC\"\n",
    "        ]\n",
    "\n",
    "        print(locs)\n",
    "\n",
    "        # add the LOC entities to the list\n",
    "        locs_all.append(locs)\n",
    "\n",
    "    return locs_all\n",
    "\n",
    "df[\"Regions\"] = ner(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b3099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d1af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3157 entries, 0 to 3156\n",
      "Data columns (total 19 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Source           3157 non-null   object \n",
      " 1   URL              3157 non-null   object \n",
      " 2   Title            3157 non-null   object \n",
      " 3   Date             3157 non-null   object \n",
      " 4   Content          3157 non-null   object \n",
      " 5   Cleaned_Title    3157 non-null   object \n",
      " 6   Cleaned_Content  3157 non-null   object \n",
      " 7   Text             3157 non-null   object \n",
      " 8   Regions          3157 non-null   object \n",
      " 9   Mode             3157 non-null   object \n",
      " 10  sadness          3157 non-null   float64\n",
      " 11  joy              3157 non-null   float64\n",
      " 12  love             3157 non-null   float64\n",
      " 13  anger            3157 non-null   float64\n",
      " 14  fear             3157 non-null   float64\n",
      " 15  surprise         3157 non-null   float64\n",
      " 16  Sentiment        3157 non-null   object \n",
      " 17  Confidence       3157 non-null   float64\n",
      " 18  Subjectivity     3157 non-null   float64\n",
      "dtypes: float64(8), object(11)\n",
      "memory usage: 468.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"analysis_ready.csv\")\n",
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084c4ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"analysis_ready.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7836b1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
